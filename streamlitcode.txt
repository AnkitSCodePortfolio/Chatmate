import streamlit as st
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from PIL import Image
import os
import base64
from audio_recorder_streamlit import audio_recorder
from io import BytesIO
import pyttsx3
 
 
def client_creator():
    AZURE_OPENAI_SERVICE = "oai-cedi-ds-swc-dev"
    azure_credential = DefaultAzureCredential()
    token_provider = get_bearer_token_provider(azure_credential, "https://cognitiveservices.azure.com/.default")
    client = AzureOpenAI(
        azure_endpoint=f"https://{AZURE_OPENAI_SERVICE}.openai.azure.com",
        api_version="2024-02-15-preview",
        azure_ad_token_provider=token_provider,
    )
    return client
 
client = client_creator()
 
def stream_llm_response(client, model_params):
    response_message = ""
 
    for chunk in client.chat.completions.create(
        model=model_params["model"] if "model" in model_params else "gpt-4o",
        messages=st.session_state.messages,
        temperature=model_params["temperature"] if "temperature" in model_params else 0.3,
        max_tokens=4096,
        stream=True,
    ):
        try:
            if chunk.choices:  # Check if choices list is not empty
                content = chunk.choices[0].delta.content
                if content:
                    response_message += content
                    yield response_message
        except IndexError as e:
            st.write(f"Debug: Exception - {str(e)}")
            st.write(f"Debug: Chunk - {chunk}")
 
    st.session_state.messages = []  # Clear messages after getting response
 
    st.session_state.messages.append({
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": response_message,
            }
        ]})
 
 
def get_image_base64(image_raw):
    buffered = BytesIO()
    image_raw.save(buffered, format=image_raw.format)
    img_byte = buffered.getvalue()
 
    return base64.b64encode(img_byte).decode('utf-8')
 
 
def main():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    logo_path = os.path.join(current_dir, "logo.png")
    logo = Image.open(logo_path)
    st.set_page_config(
        page_title="The OmniChat",
        page_icon=logo,
        layout="wide",
        initial_sidebar_state="expanded",
    )
   
 
    st.image(logo, use_column_width=False, output_format="PNG", width=150,)
    st.markdown(f"""<h1 style="text-align: center; color: #6ca395;"> <i>The OmniChat</i> üí¨</h1>""", unsafe_allow_html=True)
 
    if "messages" not in st.session_state:
        st.session_state.messages = []
 
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            for content in message["content"]:
                if content["type"] == "text":
                    st.write(content["text"])
                elif content["type"] == "image_url":      
                    st.image(content["image_url"]["url"])
 
    with st.sidebar:
        model = st.selectbox("Select a model:", [
            "gpt-4o",
        ], index=0)
       
        with st.expander("‚öôÔ∏è Model parameters"):
            model_temp = st.slider("Temperature", min_value=0.0, max_value=2.0, value=0.3, step=0.1)
 
        audio_response = st.toggle("Audio response", value=False)
        # if audio_response:
        #     cols = st.columns(2)
        #     with cols[0]:
        #         tts_voice = st.selectbox("Select a voice:", ["alloy", "echo", "fable", "onyx", "nova", "shimmer"])
        #     with cols[1]:
        #         tts_model = st.selectbox("Select a model:", ["tts-1", "tts-1-hd"], index=1)
 
        model_params = {
            "model": model,
            "temperature": model_temp,
        }
 
        def reset_conversation():
            if "messages" in st.session_state and len(st.session_state.messages) > 0:
                st.session_state.pop("messages", None)
 
        st.button("üóëÔ∏è Reset conversation", on_click=reset_conversation)
 
        st.divider()
 
            # Image Upload
        if model in ["gpt-4o"]:
                   
                st.write("### **üñºÔ∏è Add an image:**")
 
                def add_image_to_messages():
                    if st.session_state.uploaded_img or ("camera_img" in st.session_state and st.session_state.camera_img):
                        img_type = st.session_state.uploaded_img.type if st.session_state.uploaded_img else "image/jpeg"
                        raw_img = Image.open(st.session_state.uploaded_img or st.session_state.camera_img)
                        img = get_image_base64(raw_img)
                        st.session_state.messages.append(
                            {
                                "role": "user",
                                "content": [{
                                    "type": "image_url",
                                    "image_url": {"url": f"data:{img_type};base64,{img}"}
                                }]
                            }
                        )
 
                cols_img = st.columns(2)
 
                with cols_img[0]:
                    with st.expander("üìÅ Upload"):
                        st.file_uploader(
                            "Upload an image",
                            type=["png", "jpg", "jpeg"],
                            accept_multiple_files=False,
                            key="uploaded_img",
                            on_change=add_image_to_messages,
                        )
 
                with cols_img[1]:                    
                    with st.expander("üì∏ Camera"):
                        activate_camera = st.checkbox("Activate camera")
                        if activate_camera:
                            st.camera_input(
                                "Take a picture",
                                key="camera_img",
                                on_change=add_image_to_messages,
                            )
 
       
 
         # Audio Upload
        st.write("#")
        st.write("### **üé§ Add an audio:**")
 
        audio_prompt = None
        if "prev_speech_hash" not in st.session_state:
            st.session_state.prev_speech_hash = None
 
        speech_input = audio_recorder("Press to talk:", icon_size="3x", neutral_color="#6ca395", )
        if speech_input and st.session_state.prev_speech_hash != hash(speech_input):
                st.session_state.prev_speech_hash = hash(speech_input)
                transcript = client.audio.transcriptions.create(
                    model="whisper",
                    file=("audio.wav", speech_input),
                )
 
                audio_prompt = transcript.text
 
        st.divider()
       
 
 
    # Create input field for user message
    user_input = st.chat_input("Enter your message:")
 
    if user_input or audio_prompt :                                         #or audio_prompt
        st.session_state.messages.append({
            "role": "user",
            "content": [{
                "type": "text",
                "text": user_input or audio_prompt ,                         #or audio_prompt
            }]
        })
 
        with st.chat_message("user"):
            st.markdown(user_input or audio_prompt)
 
        with st.chat_message("assistant"):
            response = ""
            response_placeholder = st.empty()
            for chunk in stream_llm_response(client, model_params):
                response = chunk
                response_placeholder.markdown(response)
 
        # --- Added Audio Response (optional) ---
            if audio_response:
                engine = pyttsx3.init()
                unique_audio_file = f'response_audio_{uuid.uuid4()}.mp3'
                engine.save_to_file(response, unique_audio_file)
                engine.runAndWait()
 
                with open(unique_audio_file, 'rb') as audio_file:
                    audio_bytes = audio_file.read()
                audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')
                # audio_html = f"""<audio controls autoplay> <source src = "data:audio/mp3;base64,{audio_base64}" type = "audio/mp3"></audio>"""
                # st.markdown(audio_html, unsafe_allow_html=True)
                st.audio(audio_bytes, format='audio/mp3')
 
                # Remove the audio file after use
                os.remove(unique_audio_file)
 
           
 
 
 
                # response =  client.audio.speech.create(
                #     model=tts_model,
                #     voice=tts_voice,
                #     input=st.session_state.messages[-1]["content"][0]["text"],
                # )
                # audio_base64 = base64.b64encode(response.content).decode('utf-8')
                # audio_html = f"""
                # <audio controls autoplay>
                #     <source src="data:audio/wav;base64,{audio_base64}" type="audio/mp3">
                # </audio>
                # """
                # st.markdown(audio_html)
       
 
if __name__ == "__main__":
    main()